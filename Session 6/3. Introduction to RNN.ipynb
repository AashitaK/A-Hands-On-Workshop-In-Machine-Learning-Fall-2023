{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hands-on Workshop series in Machine Learning\n",
    "#### Instructor: Dr. Aashita Kesarwani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Drawbacks of the Bag-Of-Words (BOW) method:\n",
    "* **High dimension for input vectors**\n",
    "* **Loss of order of words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Can you think of ways to vectorize sentences while preserving the order of words?\n",
    "\n",
    "An easiest way to preserve the order or words while vectorization would be to\n",
    "* Set a certain length for input sequences, say 20 words\n",
    "* Process all the sentences in the training dataset so they consist of exactly $n$ words (say $20$ words) either by padding with null words or chopping off longer sentences\n",
    "* Create a vocabulary of all words in the training data and assign them hashes (numbers)\n",
    "* Vectorize the input sequences by replacing the words with their hashes in the vocabulary\n",
    "* Feed this input to a vanilla neural network\n",
    "\n",
    "\n",
    "The feed forward neural network with the above input might be able to capture information about the order of words in the sentence but it would need to learn all rules of the language separately at each point. For example, it would treat the following two sentences differently \n",
    "* \"I went to Hawaii in 2018.\"\n",
    "* \"In 2018, I went to Hawaii.\"\n",
    "\n",
    "The relevant information about when the narrator went to Hawaii is present in 6th and 2nd position respectively. Many more training examples would be needed to train the network to extract the relevant information as the network needs to learn the same rules separately for all positions in the input layer.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are specifically designed to work well on sequential data. They are much more efficient for textual data as compared to vanilla neural network (i.e. multi-layer perceptrons).\n",
    "\n",
    "Other than natural language processing, RNNs can be applied to time-series based data as well. \n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "* RNN takes an input of sequence, denoted as $x^{(1)}, x^{(2)}, \\dots, x^{(t)}, \\dots$, instead of a single input vector as seen previously.\n",
    "* It can handle input sequences of any length.\n",
    "* The information from the time step $t-1$ is passed on as input to the next time step $t$.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1w7enotgoPLfKKsNV-1-jZxciBkMDws9z\" width=\"800\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematics, we often study dynamical systems that involves recurrence relations:\n",
    "\n",
    "$$s^{(t)} = f(s^{(t-1)}; \\theta)$$\n",
    "\n",
    "where $s^{(t)}$ is the state of system at time $t$.\n",
    "\n",
    "These dynamical systems can also involve input signals $x^{(t)}$ at each step:\n",
    "\n",
    "$$s^{(t)} = f(s^{(t-1)}, x^{(t)} ; \\theta)$$\n",
    "\n",
    "Such a recurrence relation is involved in the architecture of most RNN (though the architecture can vary greatly as discussed below). The hidden nodes in RNN at time $t$ can be defined as:\n",
    "\n",
    "$$h^{(t)} = f(h^{(t-1)}, x^{(t)} ; \\theta)$$\n",
    "\n",
    "Here, the hidden state $h^{(t)}$ defined in terms of $h^{(t-1)}$ can be unfolded in the following manner, for say $t=3$:\n",
    "\n",
    "\\begin{align}\n",
    " h^{(3)} &= f(h^{(2)}, x^{(3)}; \\theta) \\\\ \n",
    " &= f(f(h^{(1)}, x^{(2)}; \\theta), x^{(3)}; \\theta) \\\\\n",
    " &= f(f(f(h^{(0)}, x^{(1)}; \\theta), x^{(2)}; \\theta), x^{(3)}; \\theta) \\\\\n",
    " &= g^{(3)}(x^{(1)}, x^{(2)}, x^{(3)})\n",
    "\\end{align}\n",
    "\n",
    "Thus, the unfolded recurrence can also be represented by a function $g^{(t)}$ as:\n",
    "\n",
    "\\begin{align}\n",
    " h^{(t)} &= f(h^{(t-1)}, x^{(t)}; \\theta) \\\\ \n",
    " &= g^{(t)}(x^{(1)}, \\dots, x^{(t-1)}, x^{(t)})\n",
    "\\end{align}\n",
    "\n",
    "Advantages of using the model $h^{(t)} = f(h^{(t-1)}, x^{(t)} ; \\theta)$ over $h^{(t)} = g^{(t)}(x^{(1)}, \\dots, x^{(t-1)}, x^{(t)})$:\n",
    "* The function $g^{(t)}$ can be a different function at each time step but the ***same*** transition function $f$ with the same parameters $\\theta$ can be used to define hidden state in terms of $h^{(t-1)}$ and input $x^{(t)}$ for every time step. \n",
    "* The input sequences can be of variable length but the input for the model is always the same size consisting of $h^{(t-1)}$ and input $x^{(t)}$.\n",
    "\n",
    "Thus, a single shared model $f$ can be learned that will generalize to sequences of any length and will eliminate the need to learn a separate model $g^{(t)}$ for each time step. This parameter sharing also allows us to train the model with far fewer training examples.\n",
    "\n",
    "As the RNN involve function compositions multiple times, the composite function can result in very high non-linear behaviour.\n",
    "\n",
    "Some common types of RNN architecture:\n",
    "\n",
    "### 1. Output at each step and recurrent connections between hidden units \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gnZmLW2WEkqDakNnGxy0OljT9NWBUFsD\" width=\"600\" />\n",
    "\n",
    "\n",
    "The equations for the simple RNN architecture:\n",
    "\n",
    "\\begin{align}\n",
    " a^{(t)} &= b + Wh^{(t-1)} + U x^{(t)} \\\\ \n",
    " h^{(t)} &= \\tanh (a^{(t)})\\\\\n",
    " o^{(t)} &= c + Vh^{(t)}\\\\\n",
    " \\hat{y}^{(t)} &= \\text{softmax} (o^{(t)})\\\\\n",
    "\\end{align}\n",
    "\n",
    "The activation function for the hidden unit is hyperbolic tangent which is simply sigmoid function scaled and translated along y-axis.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/980px-Hyperbolic_Tangent.svg.png?20090905154026\" width=\"450\" height=\"200\" />\n",
    "\n",
    "Cost function, $L$, is the sum of negative loglikelihood of $y^{(t)}$ given $x^{(1)}, \\dots, x^{(t-1)}, x^{(t)}$\n",
    "\n",
    "$$ L = \\sum_t L^{(t)} = - \\sum_t \\log p \\big(y^{(t)} | x^{(1)}, \\dots, x^{(t-1)}, x^{(t)} \\big)$$\n",
    "\n",
    "\n",
    "### 2. A single output at the end and recurrent connections between hidden units\n",
    "When the output is only needed at the end after reading the entire input sequence, this architecture is used.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1vkmFkE94zgkHXfiqrpcExo5HKikevoWW\" width=\"600\" />\n",
    "\n",
    "\n",
    "Some applications for a single output:\n",
    "* Sentiment analysis (Text classification)\n",
    "* Sentence completion (guessing the next word in the sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Back-propagation through time (BPTT):\n",
    "The forward propagation moves from left-to-right along the time steps whereas the backpropagation moves in the opposite direction from right-to-left. The computations for weight updates in the backpropagation cannot be parallelized for different time steps as the forward propagation is sequential and hence, the computations cannot be done for time step $t=\\tau$ unless they are done for all time steps upto $t=\\tau-1$. Thus, the RNNs can be powerful but very expensive to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "When we read and try to understand a sentence, we do not only pay attention to the current word, but we also keep the context in mind from what we read previously in the text. This dependence on previous words to understand the context for the current word is called the dependency. The RNN can learn short-term dependencies much better than long-term dependencies, meaning it remembers the context from the neighbouring previous words better but keep losing the information as we move along the sentence.\n",
    "\n",
    "Simple RNNs \n",
    "* Pros:\n",
    "    * Order/sequence of words are considered\n",
    "    * Input can be of any length\n",
    "    * RNNs were unreasonably far more effective for textual data than BOW methods in use at that time (See  [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy). RNN architectures were developed and studied since mid-1980s but their application in NLP tasks did not pick up pace until the computational power and libraries became widely accessible around 2015.\n",
    "* Cons:\n",
    "    1. Suffer from vanishing gradients, especially for longer sequences.\n",
    "    2. Not sufficiently good at long term memory in sequences.\n",
    "    3. Assume one-to-one correspondence between input and output sequences and hence, the architecture is not suitable for many common NLP tasks such as translation between languages, text summarization, question-answering, etc.\n",
    "    4. Slow to train as the training is sequential and thus, cannot be sufficiently parallelized (think in terms of being able to write vectorized code for forward and backward propagation through time steps).\n",
    "    5. Transfer learning is not very useful in application.\n",
    "\n",
    "**Transfer learning**: Using pre-trained neural networks (that were trained on a larger dataset requiring more computing resources) and fine-tuning them for a specific NLP task. The use of transfer learning makes state-of-the-art models widely accessible for users.\n",
    "\n",
    "\n",
    "LSTM and GRU are slight modifications to simple RNN to address the problem of long-term dependencies. Please read [this illustrative article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) if you learn more about LSTM. They improve the computational time and performance to some extent. \n",
    "\n",
    "\n",
    "### Encoder-decoder or sequence-to-sequence models\n",
    "\n",
    "Simple RNN architectures assumes a correspondence between each input and output sequence at each time step. For many common NLP tasks, the input and output sequences need not be of the same size and a one-to-one correspondence between input and output may be absent. For this purpose, sequence-to-sequence or encoder-decoder architecture is introduced. \n",
    "\n",
    "\n",
    "The encoder creates a numerical context vector from the entire input sequence and pass it on to the decoder to generate the output. \n",
    "\n",
    "* Encoder RNN reads input sequence, called “context” and produces a representation of the context, say C \n",
    "* Final hidden state of encoder RNN is used to compute the (generally) fixed-length context variable C, which represents a semantic summary of the input sequence\n",
    "* Context variable C is Input to the Decoder RNN\n",
    "* Decoder RNN generates output sequence \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg\" width=\"500\" height=\"70\" />\n",
    "\n",
    "The encoder and decoder can also be LSTM/GRU units instead of simple RNN units. The attention-based encoder-decoder or sequence-to-sequence models, also known as transformers, have different architecture involving an attention layer.\n",
    "\n",
    "Encoder-decoder (or sequence-to-sequence models) can be used for several NLP tasks:\n",
    "* Text generation\n",
    "* Text summarization\n",
    "* Question-Answering\n",
    "* Neural machine translation\n",
    "* Audio captioning (speech-to-text)\n",
    "* Text-to-speech conversion\n",
    "* Image captioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism\n",
    "\n",
    "The context vector creates a bottleneck in passing relevant information for longer sequences. Encoder-decoder models performs a lot better while using attention mechanism which simply passes on more information from each encoder unit to each decoder unit.\n",
    "\n",
    "<img src=\"https://blog.floydhub.com/content/images/2019/09/Slide37-1.JPG\" width=\"750\" />\n",
    "<h4 align=\"center\">\n",
    "Sequence-to-sequence model (without using attention mechanism)\n",
    "</h4>\n",
    "\n",
    "The breakthrough in the performance of deep learning for NLP came from the introduction of transformer models that use attention mechanism (from the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper) instead of recurrent connections across time steps. Transformer models completely discarded the use of both recurrence and convolution entirely. This enabled the parallelization that sped up the training process.\n",
    "\n",
    "Transformer based models such as GPT, GPT-2, and GPT-3 by OpenAI, Google's BERT and many modifications of these algorithms have shown remarkable results in most NLP tasks. Transfer learning can be used for fine-tuning pre-trained transformer models freely available online and applying them for various specific tasks in several different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-attention in transformers:\n",
    "The attention mechanism encodes the contextual information for a word in the sentence. For each word, attention scores w.r.t. other words in the sentence are calculated that corresponds to the importance of those words in understanding the current word.\n",
    "\n",
    "For example, the attention scores for \"it\" are visualized below from a trained transformer model. You can see how the attention score carry relevant contextual information.\n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
    "\n",
    "We will come back in the next class to understand attention mechanism in more detail by playing around the visualizations in this [BertViz Interactive Tutorial](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing)\n",
    "\n",
    "[BertViz repository](https://github.com/jessevig/bertviz) is an excellent resource for interactively visualizing attention in Transformer language models such as BERT, GPT2, or T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Bag-Of-Words </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Simple RNN </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> LSTM/GRU </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs and attention mechanism </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Transformers </center>\n",
    "\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformer models consist of encoders and/or decoders. \n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "In general, there are three kinds of transformer models:\n",
    "* *Auto-encoding*: Transformer models such as BERT (Bidirectional Encoder Representations from Transformers), introduced in the [paper](https://arxiv.org/pdf/1810.04805.pdf) by Devlin et al. from Google AI Language in 2019, consists of **only encoders**. Pre-trained BERT model is widely used as \"contextualized word embeddings\". This word embeddings when paired with another architecture such as Multi-Layer Perceptron or LSTM can be used for text classifications problems that we have seen earlier. \n",
    "* *Auto-regressive*: Transformers such as GPT-1/2/3/4(Generative Pre-trained Transformers series) that are language models used for text generation consists of **only decoders**.\n",
    "* *Sequence-to-sequence*: Transformers such as BART, a denoising autoencoder for pretraining sequence-to-sequence models, consists of **both encoders and decoders** and can be used for several NLP tasks such as question-answering, text summarization, etc.\n",
    "\n",
    "\n",
    "<img src=\"https://amatriain.net/blog/images/02-06.png\" width=\"600\" />\n",
    "<h4 align=\"center\">\n",
    "Timeline for Transformer models  \n",
    "</h4>\n",
    "\n",
    "(Image credit: [Xavier Amatriain](https://arxiv.org/pdf/2302.07730.pdf))\n",
    "\n",
    "The original transformer model in the paper [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) was trained on WMT 2014 English-to-German translation dataset consisting of  4.5 million pairs of phrases. We will refer to this sequence-to-sequence model as vanilla transformer. \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/llm-reading-list/transformer.png\" width=\"650\" />\n",
    "\n",
    "The above image shows a single encoder block and a single decoder block. In the original architecture, the encoder/decoder blocks are stacked on top of each other. Six blocks are used in the model trained in the paper but they can be any number. Though the six encoder blocks have the same structure, their weights are different that will be determined via the training process. The same is true for decoder blocks.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=\"600\" />\n",
    "\n",
    "#### Encoder architecture\n",
    "<img  src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-3.png\" width=\"350\" />\n",
    "\n",
    "The encoder consists of \n",
    "* Input embedding\n",
    "* Positional embedding\n",
    "* Multi-head self-attention\n",
    "* Feedforward network\n",
    "\n",
    "**Residual connections** are used to bypass the complex blocks so that the gradients can flow freely in the backward direction. Add and Norm blocks are used to merged the residual connections with other blocks.\n",
    "\n",
    "#### Decoder architecture\n",
    "\n",
    "<img  src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-17-627x1024.png\" width=\"500\" />\n",
    "\n",
    "The decoder is similar to the encoder but it has an additional masked multi-head self-attention layer for the shifted output in which only the words prior to the current word can be seen and the later words in the output sequence are hidden by the mask.\n",
    "\n",
    "The decoder also has a final linear+softmax layer for generating the output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer tree:\n",
    "https://pbs.twimg.com/media/Fuz4UrZaYAAE4ZS?format=jpg&name=large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgement\n",
    "Some content and diagrams are taken from the online book: https://www.deeplearningbook.org/contents/rnn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
