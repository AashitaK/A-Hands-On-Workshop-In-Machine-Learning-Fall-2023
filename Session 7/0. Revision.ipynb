{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b83e0a8",
   "metadata": {},
   "source": [
    "#### Word embeddings\n",
    "\n",
    "ADD CONTENT HERE!!!!!!!!!!!!\n",
    "\n",
    "----\n",
    "#### Models for textual data we studied so far\n",
    "\n",
    "<center> Bag-Of-Words </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> RNNs (Recurrent Neural Networks) </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Encoder-Decoder with RNNs and attention mechanism </center>\n",
    "$$\\Downarrow$$\n",
    "\n",
    "<center> Transformers </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1bc7a",
   "metadata": {},
   "source": [
    "#### Transformers\n",
    "\n",
    "Transformer models consist of encoders and/or decoders. \n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "The original transformer model in the paper **[Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)** was trained on WMT 2014 English-to-German translation dataset consisting of  4.5 million pairs of phrases. We will refer to this sequence-to-sequence model as vanilla transformer. \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/llm-reading-list/transformer.png\" width=\"650\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a362e",
   "metadata": {},
   "source": [
    "Three kinds:\n",
    "* *Encoders only*: BERT and its many variations used for generating word embeddings\n",
    "* *Decoders only*: Generative language models such as GPT-1/2/3/4 \n",
    "* *Encoder-decoder models*: Transformers such as BART, a denoising autoencoder for pretraining sequence-to-sequence models, that can be used for several NLP tasks such as question-answering, text summarization, language translation, etc.\n",
    "\n",
    "Full forms:\n",
    "* BERT: Bidirectional Encoder Representations from Transformers\n",
    "* GPT: Generative Pre-trained Transformers\n",
    "* BART: Bidirectional and Auto-Regressive Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d8571",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/media/Fuz4UrZaYAAE4ZS?format=jpg&name=large\" width=\"700\" />\n",
    "<h4 align=\"center\">\n",
    "Evolutionary Tree for LLMs (Large Language Models)  \n",
    "</h4>\n",
    "\n",
    "[Image](https://pbs.twimg.com/media/Fuz4UrZaYAAE4ZS?format=jpg&name=large) credit: [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond by Yang et al.](https://arxiv.org/pdf/2304.13712.pdf)\n",
    "\n",
    "The biggest advantage of Transformers over earlier architectures in neural networks, esp. involving recurrence in neural networks, is that they are highly parallelizable. This makes them computationally faster to train and led to the rise of models consisting of billions or trillions of parameters. More parameters means more information can be stored in these models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
